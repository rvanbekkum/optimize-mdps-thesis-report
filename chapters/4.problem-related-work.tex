\chapter{Problem Formulation and Related Work}
\label{ch:problem-related-work}


\section{Learning SDM Models from Execution Traces}
\label{sec:learning-models}

In \acrshort{acr:dtp} it is common to make use of a (compact) probabilistic model of the environment.
However, devising an optimal model for a specific system or process can be a daunting and error-prone task, especially with many possibilities while wanting a compact model and computation cost efficient.....

% TODO;s
%% Fitting Markov Chains/MDPs:
	% Refer back to section 2.2.1:
	% - likelihood maximization
	% - Bayesian inference/optimization
%% Fitting HMMs/POMDPs:
	% ITERATIVE ADJUSTMENT OF PROBABILITIES:
	% - Baum-Welch Algorithm
	% - Gradient-Ascent in Likelihood
	% STATE MERGING
	% - Best-first
	% - Trajectory clustering
%% Reinforcement Learning: Online Model Learning

%% However, most of the previous works assume the state-space of the probabilistic model to be known.

%\section{Bayesian Reinforcement Learning}
%\label{sec:bayesian-reinforcement-learning}

% 

%
%\section{Overview}
%\label{sec:related-work-overview}
%
%% 