\chapter{Related Work}
\label{ch:problem-related-work}

A common approach in \acrshort{acr:dtp}, as was discussed in \autoref{ch:introduction}, is to formulate plans by making use of (compact) probabilistic models of the systems under consideration.
In particular we focus our attention to devising \acrshort{acr:mdp} models, which need to accurately define what constitutes the state of the system, reflect uncertainty through transition probabilities, and specify goals through rewards.
The problem we face is that handcrafting accurate, well-generalizing models is a difficult and time-costly process for a human designer.
To overcome this problem the goal is to automatize the model development process by applying learning algorithms on data that describes the dynamics of the system under consideration.
Therefore, this chapter aims to provide some insight into the existing approaches for automating the development of probabilistic models for planning under uncertainty and the existing alternatives for automated control of systems involving uncertainty.

First \autoref{sec:learning-probabilistic-models} gives an overview of the existing algorithms for learning models from execution traces of the system.
Then, in \autoref{sec:active-learning} a number of approaches are examined, which combine offline methods for learning (initial) models with \acrshort{acr:rl} algorithms which update the model parameters online.
Finally, \autoref{sec:mdp-uncertain-probabilities} looks into approaches that extend the traditional \acrshort{acr:mdp} framework by accounting for uncertainty in the transition probabilities, with the aim to overcome the infeasibility of obtaining completely accurate probabilistic models.

%\section{Problem Formulation}
%\label{sec:problem-formulation}
%
%In \acrshort{acr:dtp} it is common to make use of a (compact) probabilistic model of the environment.
%However, devising an optimal model for a specific system or process can be a daunting and error-prone task, when considering the wide range of possibilities while wanting a compact and computation-cost efficient model.

\section{Learning Probabilistic Models from Execution Traces}
\label{sec:learning-probabilistic-models}

To facilitate the learning of system models, the first step is to obtain a dataset which describes the dynamics of the system under consideration.
A common approach of obtaining this data is through the recording of observations made in an exploration phase in which the system aims to gather information about the effect of performing various actions in different situations.
This exploration could be done selecting actions at random, executing a specific policy, or following another method of choosing actions.
The next step is to learn a probabilistic model which most accurately explains the execution traces obtained in this exploration phase.
In this section a number of the algorithms for this purpose are considered, where \autoref{sec:baum-welch} evaluates the well-known \textit{Baum-Welch} algorithm that iteratively updates transition and emission probabilities starting from a model with initial estimates of these probabilities, while \autoref{sec:state-merging} addresses a class of algorithms based on merging (time-)states for developing \acrshortpl{acr:mdp}.

\subsection{The Baum-Welch Algorithm}
\label{sec:baum-welch}

The most widely applied approaches for learning probabilistic models are based on maximizing the likelihood of observing the execution traces of the dataset.
When the goal is to learn the parameters of a Markov Chain, the transition probabilities can be estimated as we have seen in \autoref{sec:markov-chains}, by the ratio of the times a particular transition is observed and the total number of transitions observed from the corresponding starting state.
However, while Markov Chains are similar to \acrshortpl{acr:mdp} in that they describe the evolution of a stochastic process over time, they do not involve decisions on actions to perform.
Due to the close nature of these probabilistic models the likelihood maximization can be applied almost directly to learn the transition probabilities of \acrshortpl{acr:mdp}, although requires more data due to the addition of actions.

In practice, it is more common to only have a sequence of observations at one's disposal which do not directly map to the states of the system.
The approach of likelihood maximization generalizes to learning partially observable Markov Models through the well-known \textit{Baum-Welch algorithm} \cite{welch2003hidden}, which is a special instance of the \acrfull{acr:em} algorithm generally used for learning the parameters of \acrshortpl{acr:hmm}.
Given a discrete or continuous observation space $\mathcal{V}$, a discrete state space $\mathcal{S}$, and a set $\mathcal{O}$ of i.i.d. observation sequences, the algorithm learns a model $\mathcal{M}^\ast = \argmax_\mathcal{M} p(\mathcal{O}\vert\mathcal{M})$.
Informally, this is done by first providing manual estimates of the model parameters and then iteratively computing the expectations of how frequently the transitions and emissions are used (i.e., the \textit{E-step}) and updating these parameters based on those computed expectations (i.e., the \textit{M-step}).
For a more detailed explanation of the Baum-Welch algorithm one may refer to the tutorial by \citeauthor{bilmes1998gentle} in \cite{bilmes1998gentle}.

Although the approach turns out to work quite well for learning \acrshortpl{acr:hmm} for recognition purposes, some problems emerge when applying the algorithm to learn \acrshortpl{acr:pomdp} for planning under uncertainty.
That is, the algorithm is well known to be sensitive to its initialization (particularly in the case of continuous observations) and depending on the choice of the initial model parameters it may result in local maxima.
However, to some extent this problem can be overcome by segmenting the observation sequences using $k$-Means clustering to restrict the model to a discrete \acrshort{acr:hmm} or \acrshort{acr:pomdp} (e.g., see \cite{calinon2007learning}).
This discretization however, depending on the choice of the hyperparameter(s) of the clustering algorithm, might neglect some valuable distinctions between observations or states of the system.

A particularly relevant work is that of \citeauthor{shatkay1997learning} \cite{shatkay1997learning} in which the algorithm is applied to learn \acrshort{acr:pomdp} models in the context of mobile robot navigation.
In their work they make the assumption of a finite set of states whose size is known, and associate each state with a point in some metric space derived from the odometric ability of the robot.
Based on a set of gathered odometric readings an initial \textit{topological} model is learned by applying $k$-Means clustering, taking the clusters as the states in which the observations were made, and based on state and observation counts make initial estimates of the model parameters.
Then, an adapted version of Baum-Welch is applied which takes into account odometric information to iteratively update the model.
The obtained results are compared to those that emerge from the traditional Baum-Welch algorithm, which is used to demonstrate that exploiting odometric information can reduce the number of iterations and improve the final model.

Similarly, an adaptation of the Baum-Welch algorithm is applied in \cite{koenig1996unsupervised} to learn \acrshortpl{acr:pomdp} which exploits prior knowledge of map symmetry and does not adjust probabilities that are assumed to be approximately correct since the sensor and actuator models are expected to be similar in different environments.
In this way the amount of required training data is restricted and updating the model happens more selectively and efficiently, although it demands the need for handcrafting an initial topological model and defining constraints on the model structure.

% Explain drawbacks of Baum-Welch for planning: e.g. local maxima
% https://www.google.nl/url?sa=t&rct=j&q=&esrc=s&source=web&cd=9&cad=rja&uact=8&ved=0ahUKEwiZpvCGmcLTAhXESBQKHTq7Bg0QFghiMAg&url=http%3A%2F%2Fciteseerx.ist.psu.edu%2Fviewdoc%2Fdownload%3Fdoi%3D10.1.1.57.4643%26rep%3Drep1%26type%3Dps&usg=AFQjCNH-b5YR7gExaSuktykOrezijr8kZw&sig2=f4NrRWV0O8dSokuqWWCP7Q
% @techreport{shatkay1997learning,
% title={Learning hidden Markov models with geometric information},
% author={Shatkay, Hagit and Kaelbling, Leslie Pack},
% year={1997},
% institution={Citeseer}
% }

\subsection{State Merging Algorithms}
\label{sec:state-merging}

A completely different class of algorithms is based on the merging of states to acquire probabilistic models.
The type of algorithms that are considered here are based on a method called \textit{Best-First Model Merging} which was first proposed by \citeauthor{stolcke1994best} in \cite{stolcke1994best} to learn \acrshortpl{acr:hmm} for the purpose of speech recognition.

Given a training set $\mathcal{O}$ of i.i.d. observation sequences, this algorithm starts from an initial \acrshort{acr:hmm} whose state space consists of separate states for each time-step in the sequences.
The entries of the transition and emission matrix are thus initialized with probability one for each of the transitions between subsequent states and each percept in the observation sequence respectively.
Although this \acrshort{acr:hmm} explains the data perfectly it is usually not efficient and therefore the model is iteratively updated by merging those pairs of states that decrease the likelihood the least.
This repeats itself until a stop criterion is reached, such as the likelihood falling below a certain value or reaching a predefined number of states.

This approach can be adapted for learning \acrshortpl{acr:pomdp} as described in \cite{nikovski1999learning} such that the transition function is initially incomplete as it will only be defined for the actions corresponding to the transitions between subsequent states.
A particular advantage of this approach is that one can choose to adapt the merging criterion so that for instance states are only merged if they both map to the same goal percept.
However, a major disadvantage of the approach is that alternative merges are never considered by the algorithm, although in the end they may result in better models.
This means that the approach is prone to local maxima, although this can be avoided to some extent by considering a set of most promising merges in each iteration, although it would make the algorithm even more cost-expensive than it already is.

\begin{algorithm}[t]
	\caption{State Merging by Trajectory Clustering}
	\label{alg:trajectory-clustering}
	\begin{algorithmic}[1]
		\Require{Training set $\mathcal{O} = \{O_1, \ldots, O_k\}$, Clustering algorithm, hyperparameter(s) $\theta$, actions $A$}
		\Ensure{\acrshort{acr:mdp} $\mathcal{M} = (\mathcal{S}, s_0, A, \delta, R)$} \Comment{$s_0$ and $R$ are here irrelevant}
		\State Compute similarities of time-states based on trajectories
		\State Cluster most similar time-states to obtain the state space $\mathcal{S}$
		\State Compute the transition probabilities to obtain $\delta$
		\State \Return \acrshort{acr:mdp} $\mathcal{M}$
	\end{algorithmic}
\end{algorithm}

A variation of the algorithm is a method seen in \cite{nikovski1999learning} that goes under the name of \textit{State Merging by Trajectory Clustering}, which makes the state merging approach more resilient to \textit{perceptual aliasing}.
In this approach the merging criterion is changed such that in each iteration those pairs of states are merged whose prior and posterior trajectories in the training set are most similar.
The pseudocode of this method is shown in \autoref{alg:trajectory-clustering} where the similarity of states is thus defined by the length of the common trajectories from and towards the states. 
In case the observations have an underlying metric space, one can choose to apply algorithms like $k$-Means clustering to cluster the most similar time-states, or alternatively turn to clustering methods that work on similarity matrices.


%One of the main issues faced in learning \acrshort{acr:mdp} models is that of deciding on the state space, while one typically does not know the size of the true state space of the real-world process that generated the training data.
%In fact this state space might even have been continuous, while the state space of the model is typically discrete.
%In particular this might form an issue in the planning phase where a reward function needs to be defined to express the goals in the \acrshort{acr:mdp}, for which there may not be a clear one-to-one mapping from real-world states to model states.

%\blindtext
%
%\subsection{State Merging by Trajectory Clustering}
%\label{sec:trajectory-clustering-merging}
%
%\blindtext


%\showoutline{
%
%\noindent\textbf{Iterative Adjustment of Probabilities}:
%\begin{itemize}
%	\item Likelihood maximization for Markov Chains/MDPs, which generalizes to Baum-Welch for HMMs/POMDPs
%	\item Gradient-Ascent
%\end{itemize}
%
%\begin{figure}[]
%	\centering
%	\includegraphics[width=0.8\textwidth]{model-merging-bfmm}
%	\caption{State Merging (Not sure yet whether this should be here)}	% TODO
%	\label{fig:state-merging}
%\end{figure}

%\noindent \textbf{State Merging}:
%\begin{itemize}
%	\item Best-First Model Merging \cite{stolcke1994best}
%	\item State Merging by Trajectory Clustering \cite{nikovski2000learning}
%\end{itemize}
%
%\noindent Other (offline/'data-based') approaches
%
%}

\section{Active Learning}
\label{sec:active-learning}

Devising a completely accurate specification of the \acrshort{acr:mdp}'s transition probabilities typically turns out infeasible for real-world \acrshort{acr:dtp} problems.
However, it should be noted that the learning algorithms that have been addressed earlier, usually recover enough to let an \acrshort{acr:mdp} reflect the actual environment in which an agent operates.
The plans that are derived accordingly from these models yield better performance than a random choice of actions in the execution of the system would.
A class of approaches that we consider in this section are those that combine offline model learning and planning methods with online (reinforcement) learning methods which incrementally update the model parameters.

\subsection{Model-Based Reinforcement Learning}
\label{sec:model-based-reinforcement-learning}

In \acrshort{acr:rl} a distinction exists between model-free and model-based methods, where in the former the choice of action is made based on previously realized value, while the latter evaluates candidate actions based on their expected future rewards.
Model-free or \textit{direct} \acrshort{acr:rl} can therefore be viewed as corresponding to habitual behavior, while model-based \acrshort{acr:rl} corresponds to goal-directed behavior.
When the system under consideration encompasses considerable structure and its dynamics can be modeled accurately, model-based approaches tend to use experiential data more efficiently, be more resilient to changing goals and are better capable of obtaining near-optimal policies \cite{atkeson1997}.
The \acrshort{acr:sdm} problems involving these type of systems are those that qualify for applying model-based \acrshort{acr:rl} in which an initial \acrshort{acr:mdp} could be obtained by offline model learning.
However, one should note that model-free \acrshort{acr:rl} approaches are better suited for learning from scratch (i.e., \acrshort{acr:sdm} problems with very limited prior knowledge and data), as model-based \acrshort{acr:rl} approaches tend to suffer from model bias \cite{deisenroth2011pilco}.
That is, model-based \acrshort{acr:rl} makes the inherent assumption that the underlying model sufficiently accurately reflects the real-world environment.

\newpage

To improve the estimations of the model parameters in model-based \acrshort{acr:rl}, the algorithms demand for efficient exploration, for which various approaches exist that are optimistic in face of uncertainty.
An example is the $E^3$ algorithm \cite{kearns2002near} which explicitly chooses between exploiting the known part of the \acrshort{acr:mdp} and optimally reaching the part that needs to be explored. 
Its exploration is based on \textit{balanced wandering}, which means that the agent will prefer the action that has been executed the least from its current state (breaking ties randomly) until the state has been visited a sufficient number of times.
Another algorithm is the $R$-Max algorithm \cite{Brafman2002}, which has a built-in mechanism for switching between exploitation and exploration and differs in its exploration by letting the agent make the assumption that a maximum possible reward can be obtained from unexplored state-action pairs.

Another issue that should be taken care of in model-based \acrshort{acr:rl} is when and how to update the \acrshort{acr:mdp} and its plan based on acquired experience. A simple approach would be to update the plan on each model update applying a well-known algorithm like \acrshort{acr:vi}, although this is computationally expensive.
To gain efficiency, frameworks like \textsc{RTMBA} \cite{hester2012rtmba} or \textsc{DYNA} and its various extensions \cite{silver2008sample}, aim to update their plans more intelligently (e.g., by updating value functions only for a subset of state-action pairs).
A different approach could be to limit exhaustive exploration of states in the first place, exemplified by the \textsc{RL-DT} algorithm proposed in \cite{hester2010generalized}, which generalizes the effects of actions across states.
This however, is particularly useful for these domains that require sample-efficient learning and for which the available time for exploration is limited.

\subsection{Active Reinforcement Learning}
\label{sec:active-reinforcement-learning}

An appealing approach to overcome inaccuracies of offline learned probabilistic models is to use them as prior models for the model-based \acrshort{acr:rl} algorithms shown in \autoref{sec:model-based-reinforcement-learning}.
In \cite{epshteyn2008active} an approach referred to as \textit{Active \acrlong{acr:rl}} is presented which uses a provided \acrshort{acr:mdp} specification as prior knowledge for exploration through model-based \acrshort{acr:rl}.
Rather than using this \acrshort{acr:mdp} for planning, its model parameters are used as a blueprint for exploration in the actual environment.
The algorithm guides the exploration by selecting these actions for which is known that the `prior' \acrshort{acr:mdp} is most sensitive to changes in the corresponding transition probabilities and rewards.


%\showoutline{
%	\begin{itemize}
%		\item Mention Model-Based \acrshort{acr:rl} or Online Model Learning
%		\item Refer to approach of combining offline and online planning: using an offline-obtained model and policy, apply model-based \acrshort{acr:rl} to update the parameters \cite{epshteyn2008active}
%		\item .....
%	\end{itemize}
%}

\subsection{Bayesian Reinforcement Learning}
\label{sec:bayesian-reinforcement-learning}

\begin{algorithm}[t]
	\caption{Model-Based Bayesian Reinforcement Learning}
	\label{alg:bayesian-reinforcement-learning}
	\begin{algorithmic}[1]
		\Require{Initial \acrshort{acr:mdp} $\mathcal{M} = (\mathcal{S}, s_0, A, \delta, R)$}
		%\Ensure{\acrshort{acr:mdp} $\mathcal{M} = (\mathcal{S}, s_0, A, \delta, R)$} \Comment{$s_0$ and $R$ are here irrelevant}
		\State Initialize prior distribution(s) over unknown parameter(s)
		\Repeat
		\State Select action $a \in A$ based on the posterior distribution(s)
		\State Execute action $a$
		\State Record observed reward and state
		\State Update posterior of unknown parameters based on observations
		\Until Agent terminates
	\end{algorithmic}
\end{algorithm}

Considerable work has been conducted on investigating the application of Bayesian methods to \acrshort{acr:rl} motivated by their close relation to decision theory for making decisions under uncertainty. 
In \textit{\acrfull{acr:brl}} \cite{ghavamzadeh2015bayesian} the idea is to learn a model of the environment by considering the unknown parameters of the model as random variables.
Over these random variables a prior distribution $p(\theta)$ is defined corresponding to the prior beliefs over the unknown parameters $\theta$ of the model.
A common choice for discrete-state and action \acrshortpl{acr:mdp} is a multinomial \textit{Dirichlet} prior, which is parameterized by a count~vector~$\Phi = (\phi_1, \ldots, \phi_k)$ which defines the number of observations of all possible transitions.
Then, as evidence is gathered, the posterior on $\theta$ (which quantifies the uncertainty over the parameters) is updated based on Bayes' rule from which an estimation of the model parameters can be obtained.
A framework that is accordingly used in model-based \acrshort{acr:brl} is the \textit{Bayes-Adaptive \acrshort{acr:mdp}} \cite{guez2012efficient}, which augments the original state space $\mathcal{S}$, transition function $\delta$ and reward function $R$ by jointly combining them with the posterior's hyperparameters $\Phi$.
A general formulation of the steps of model-based \acrshort{acr:brl} is shown in \autoref{alg:bayesian-reinforcement-learning} \cite{png2011bayesian}.

A major drawback of model-based \acrshort{acr:brl} is that obtaining exact solutions appears computationally intractable.
Nonetheless various approximate \acrshort{acr:brl} methods have been devised to provide feasible solutions (such as Bayesian DP, BOSS and Monte-Carlo tree search approaches), of which a majority is summarized in \cite{ghavamzadeh2015bayesian} for the interested reader.
Still, a major challenge is scaling \acrshort{acr:brl} methods to large-scale \acrshortpl{acr:mdp} and models with continuous spaces.
%TODO Maybe rewrite these sentences

%\showoutline{
%\begin{itemize}
%	\item Bayesian Inference (see section 2.2.1)
%	\item BRL: Maintaining a posterior distribution over possible model parameters
%	\item Framework description: \cite{png2011bayesian}
%	\begin{itemize}
%		\item Initialize distributions over unknown parameters (defining a prior, typically Dirichlet distribution)
%		\item Select action based on distributions
%		\item Execute action
%		\item Observe resulting reward and observation
%		\item Update posterior counts of unknown parameters based on observed information
%	\end{itemize}
%	\item MEDUSA, BEETLE + Other examples?
%	\item ..... (To be determined)
%\end{itemize}
%}

% https://people.eecs.berkeley.edu/~avivt/BRLS_journal.pdf

\section{MDP Models with Transition Probability Uncertainty}
\label{sec:mdp-uncertain-probabilities}

Another method to overcome the infeasibility of obtaining completely accurate specifications of transition probabilities is to extend the models and planning algorithms to take imprecise probabilities into account.
Over the years various frameworks have been introduced that utilize this idea, such as the \textit{\acrfull{acr:mdpip}} first introduced in \cite{satia1973markovian} which describes the transition probabilities as a set of linear inequalities to represent incomplete, ambiguous or conflicting beliefs over these probabilities.
That is, in this framework the probability of moving from a state $s_i$ to $s_j$ with action $a$ could for instance be expressed by a variable $p_{ij}$ s.t. a constraint $0 \leq p_{ij} \leq 0.5$.
An illustrative example of a possible application of such a framework would be that of \acrshortpl{acr:mdp} for traffic light control, for which the estimation of lane-turning probabilities for all traffic lanes is problematic, especially as they may be different over the day or throughout the year \cite{delgado2011efficient}.
For these type of domains in particular, the \acrshort{acr:mdpip} framework was devised to account for strict uncertainty in the transition function.
The framework also allows one to find plans under either optimistic or pessimistic assumptions about the true transition function.
However, while the traditional \acrshort{acr:mdpip} framework demands solution techniques that are notably time-expensive, in practice factored \acrshort{acr:mdpip} representations are used for efficient planning, for instance by synchronous dynamic programming based on parameterized algebraic decision diagrams (PADDs) \cite{delgado2011efficient} or \acrshort{acr:rtdp} solutions \cite{delgado2016real}.

An alternative framework is the \textit{\acrfull{acr:bmdp}} where a range of the transition probabilities and rewards are known, such that it can be viewed as a set of possible \acrshortpl{acr:mdp}.
In fact, \acrshortpl{acr:bmdp} are a special case of \acrshortpl{acr:mdpip} where the attainable probabilities can be defined by a discrete set rather than linear constraints, but where ranges can also be defined on the rewards.
When it is possible to restrict the parameters of the \acrshort{acr:mdp} at intervals as is done in the \acrshort{acr:bmdp} framework, the algorithms that exist for \acrshortpl{acr:bmdp} exploit the restricted structure of the parameters more efficiently than \acrshortpl{acr:mdpip}, because the \acrshort{acr:mdpip} solutions demand \acrshort{acr:lp} approaches in each iteration.
There exist solution techniques for \acrshortpl{acr:bmdp} that can efficiently learn optimal policies, such as \textit{interval \acrshort{acr:vi}} \cite{givan2000bounded} or robust versions of \acrshort{acr:rtdp} \cite{buffet2005robust}.


%\section{Gaussian Process Dynamics}
%\label{sec:gp-dynamics}
%http://machinelearning.wustl.edu/mlpapers/paper_files/ICML2011Deisenroth_323.pdf

% TODO;s
%% Fitting Markov Chains/MDPs:
	% Refer back to section 2.2.1:
	% - likelihood maximization
	% - Bayesian inference/optimization
%% Fitting HMMs/POMDPs:
	% ITERATIVE ADJUSTMENT OF PROBABILITIES:
	% - Baum-Welch Algorithm
	% - Gradient-Ascent in Likelihood
	% STATE MERGING
	% - Best-first
	% - Trajectory clustering
%% Reinforcement Learning: Online Model Learning

%% However, most of the previous works assume the state-space of the probabilistic model to be known.

%\section{Bayesian Reinforcement Learning}
%\label{sec:bayesian-reinforcement-learning}

% 

%
%\section{Overview}
%\label{sec:related-work-overview}
%
%% 