@incollection{bacciu2015probabilistic,
	title={Probabilistic Modeling in Machine Learning},
	author={Bacciu, Davide and Lisboa, Paulo JG and Sperduti, Alessandro and Villmann, Thomas},
	booktitle={Springer Handbook of Computational Intelligence},
	pages={545--575},
	year={2015},
	publisher={Springer}
}

@inproceedings{baker1992large,
	title={Large Vocabulary Recognition of Wall Street Journal Sentences at Dragon Systems},
	author={Baker, James and Baker, Janet and Bamberg, Paul and Bishop, Kathleen and Gillick, Larry and Helman, Vera and Huang, Zezhen and Ito, Yoshiko and Lowe, Stephen and Peskin, Barbara and others},
	booktitle={Proceedings of the workshop on Speech and Natural Language},
	pages={387--392},
	year={1992},
	organization={Association for Computational Linguistics}
}

@BOOK{barberBRML2012,
	author = {Barber, D.},
	title= {{Bayesian Reasoning and Machine Learning}},
	publisher = {{Cambridge University Press}},
	year = 2012}


@article{Boutilier1999,
	abstract = {Planning under uncertainty is a central problem in the study of automated sequential decision making, and has been addressed by researchers in many different fields, including AI planning, decision analysis, operations research, control theory and economics. While the assumptions and perspectives adopted in these areas often differ in substantial ways, many planning problems of interest to researchers in these fields can be modeled as Markov decision processes (MDPs) and analyzed using the techniques of decision theory. This paper presents an overview and synthesis of MDP-related methods, showing how they provide a unifying framework for modeling many classes of planning problems studied in AI. It also describes structural properties of MDPs that, when exhibited by particular classes of problems, can be exploited in the construction of optimal or approximately optimal policies or plans. Planning problems commonly possess structure in the reward and value functions used to describe performance criteria, in the functions used to describe state transitions and observations, and in the relationships among features used to describe states, actions, rewards, and observations.},
	archivePrefix = {arXiv},
	arxivId = {1105.5460},
	author = {Boutilier, Craig and Dean, Thomas and Hanks, Steve},
	doi = {doi:10.1613/jair.575},
	eprint = {1105.5460},
	isbn = {90-5199-237-8},
	issn = {10769757},
	journal = {Journal of Artificial Intelligence Research},
	pages = {1--94},
	title = {{Decision-Theoretic Planning: Structural Assumptions and Computational Leverage}},
	volume = {11},
	year = {1999}
}

@article{Brafman2002,
	abstract = {Presents R-Max: a simple model-based RL algorithm, achieving near-optimal average reward in polynomial time. Initialised optimistically, to think all unknown transitions return maximal reward and move system to a default state. Handles explore/exploit trade-off implicitly. Works on stochastic games, being a transition system of two-player matrix games (as a general case of MDPs).},
	author = {Brafman, Ronen I and Tennenholtz, Moshe},
	doi = {10.1162/153244303765208377},
	file = {:C$\backslash$:/Users/Rob/TU Delft/Thesis/Papers/MDP Model Learning/R-MAX - A General Poly-time Algorithm for Near-Optimal RL.pdf:pdf},
	isbn = {1532-4435},
	issn = {15324435},
	journal = {Journal of Machine Learning Research},
	keywords = {decision processes,learning games,markov,provably efficient learning,reinforcement learning,stochastic games},
	mendeley-groups = {MDP Model Learning},
	pages = {213--231},
	title = {{R-MAX -- A General Polynomial Time Algorithm for Near-Optimal Reinforcement Learning}},
	volume = {3},
	year = {2002}
}

@article{caelli2001shape,
	title={Shape tracking and production using hidden Markov models},
	author={Caelli, Terry and McCabe, Andrew and Briscoe, Garry},
	journal={International Journal of Pattern Recognition and Artificial Intelligence},
	volume={15},
	number={01},
	pages={197--221},
	year={2001},
	publisher={World Scientific}
}

@article{cochran2001generic,
	title={Generic Markov models for availability estimation and failure characterization in petroleum refineries},
	author={Cochran, Jeffery K and Murugan, Arvindh and Krishnamurthy, Vijayalakshmi},
	journal={Computers \& Operations Research},
	volume={28},
	number={1},
	pages={1--12},
	year={2001},
	publisher={Elsevier}
}

@article{cronvall2009combining,
	title={Combining discrete-time Markov processes and probabilistic fracture mechanics in RI-ISI risk estimates},
	author={Cronvall, Otso and M{\"a}nnist{\"o}, Ilkka},
	journal={International Journal of Pressure Vessels and Piping},
	volume={86},
	number={11},
	pages={732--737},
	year={2009},
	publisher={Elsevier}
}

@article{Degris2010,
	author = {Degris, Thomas and Sigaud, Oliver},
	file = {:C$\backslash$:/Users/Rob/TU Delft/Thesis/Papers/Factored Markov Decision Processes.pdf:pdf},
	journal = {Markov Decision Processes in Artificial Intelligence},
	pages = {99--126},
	title = {{Factored Markov Decision Processes}},
	year = {2010}
}

@article{el2008optimal,
	title={Optimal design of a cogeneration plant for power and desalination taking equipment reliability into consideration},
	author={El-Nashar, Ali M},
	journal={Desalination},
	volume={229},
	number={1},
	pages={21--32},
	year={2008},
	publisher={Elsevier}
}


@article{gales2008application,
	title={The application of hidden Markov models in speech recognition},
	author={Gales, Mark and Young, Steve},
	journal={Foundations and trends in signal processing},
	volume={1},
	number={3},
	pages={195--304},
	year={2008},
	publisher={Now Publishers Inc.}
}

@article{Ghahramani2000,
	abstract = {We introduce a new statistical model for time series that iteratively segments data into regimes with approximately linear dynamics and learnsthe parameters of each of these linear regimes. This model combines and generalizes two of the most widely used stochastic time-series models -- hidden Markov models and linear dynamical systems -- and is closely related to models that are widely used in the control and econometrics literatures. It can also be derived by extending the mixture of experts neural network (Jacobs, Jordan, Nowlan, {\&} Hinton, 1991) to its fully dynamical version, in which both expert and gating networks are recurrent. Inferring the posterior probabilities of the hidden states of this model is computationally intractable, and therefore the exact expectation maximization (EM) algorithm cannot be applied. However, we present a variational approximation that maximizes a lower bound on the log-likelihood and makes use of both the forward and backward recursions for hidden Markov models and the Kalman filter recursions for linear dynamical systems. We tested the algorithm on artificial data sets and a natural data set of respiration force from a patient with sleep apnea. The results suggest that variational approximations are a viable method for inference and learning in switching state-space models.},
	author = {Ghahramani, Z and Hinton, Geoffrey E.},
	doi = {10.1162/089976600300015619},
	file = {:C$\backslash$:/Users/Rob/TU Delft/Thesis/Papers/Variational Learning for Switching State-Space Models.pdf:pdf},
	isbn = {0899-7667 (Print)$\backslash$r0899-7667 (Linking)},
	issn = {0899-7667},
	journal = {Neural computation},
	number = {4},
	pages = {831--864},
	pmid = {10770834},
	title = {{Variational learning for switching state-space models.}},
	volume = {12},
	year = {2000}
}

@article{grimshaw2011markov,
	title={Markov chain models for delinquency: Transition matrix estimation and forecasting},
	author={Grimshaw, Scott D and Alexander, William P},
	journal={Applied Stochastic Models in Business and Industry},
	volume={27},
	number={3},
	pages={267--279},
	year={2011},
	publisher={Wiley Online Library}
}

@article{lee1968maximum,
	title={Maximum likelihood and Bayesian estimation of transition probabilities},
	author={Lee, T Cv and Judge, GG and Zellner, Arnold},
	journal={Journal of the American Statistical Association},
	volume={63},
	number={324},
	pages={1162--1179},
	year={1968},
	publisher={Taylor \& Francis Group}
}


@article{Minka1999,
	author = {Minka, T},
	file = {:C$\backslash$:/Users/Rob/TU Delft/Thesis/Papers/From Hidden Markov Models to Linear Dynamical Systems.pdf:pdf},
	journal = {Technical report, MIT},
	pmid = {1000198457},
	title = {{From Hidden Markov Models to Linear Dynamical Systems}},
	url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.51.1207{\&}rep=rep1{\&}type=pdf},
	year = {1999}
}

@article{minka2003bayesian,
	title={Bayesian inference, entropy, and the multinomial distribution},
	author={Minka, Tom},
	journal={Online tutorial},
	year={2003}
}

@incollection{parent1991stochastic,
	title={Stochastic modeling of a water resource system: analytical techniques versus synthetic approaches},
	author={Parent, E and Lebdi, F and Hurand, P},
	booktitle={Water resources engineering risk assessment},
	pages={415--434},
	year={1991},
	publisher={Springer}
}

@article{pasanisi2012estimating,
	title={Estimating discrete Markov models from various incomplete data schemes},
	author={Pasanisi, Alberto and Fu, Shuai and Bousquet, Nicolas},
	journal={Computational Statistics \& Data Analysis},
	volume={56},
	number={9},
	pages={2609--2625},
	year={2012},
	publisher={Elsevier}
}


@article{rabiner1989tutorial,
	title={A tutorial on hidden Markov models and selected applications in speech recognition},
	author={Rabiner, Lawrence R},
	journal={Proceedings of the IEEE},
	volume={77},
	number={2},
	pages={257--286},
	year={1989},
	publisher={IEEE}
}

@article{teodorescu2009maximum,
	title={Maximum Likelihood Estimation for Markov Chains},
	author={Teodorescu, Iuliana},
	journal={arXiv preprint arXiv:0905.4131},
	year={2009},
	publisher={Citeseer}
}