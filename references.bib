@article{Boutilier1999,
	abstract = {Planning under uncertainty is a central problem in the study of automated sequential decision making, and has been addressed by researchers in many different fields, including AI planning, decision analysis, operations research, control theory and economics. While the assumptions and perspectives adopted in these areas often differ in substantial ways, many planning problems of interest to researchers in these fields can be modeled as Markov decision processes (MDPs) and analyzed using the techniques of decision theory. This paper presents an overview and synthesis of MDP-related methods, showing how they provide a unifying framework for modeling many classes of planning problems studied in AI. It also describes structural properties of MDPs that, when exhibited by particular classes of problems, can be exploited in the construction of optimal or approximately optimal policies or plans. Planning problems commonly possess structure in the reward and value functions used to describe performance criteria, in the functions used to describe state transitions and observations, and in the relationships among features used to describe states, actions, rewards, and observations.},
	archivePrefix = {arXiv},
	arxivId = {1105.5460},
	author = {Boutilier, Craig and Dean, Thomas and Hanks, Steve},
	doi = {doi:10.1613/jair.575},
	eprint = {1105.5460},
	isbn = {90-5199-237-8},
	issn = {10769757},
	journal = {Journal of Artificial Intelligence Research},
	pages = {1--94},
	title = {{Decision-Theoretic Planning: Structural Assumptions and Computational Leverage}},
	volume = {11},
	year = {1999}
}
